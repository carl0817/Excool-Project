{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "from nilearn import glm\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import h5py\n",
    "import scipy\n",
    "from scipy.io import loadmat \n",
    "from scipy.io import savemat\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layername = ['/conv1','/conv2','/conv3','/conv4','/conv5','/fc6','/fc7','/fc8']\n",
    "dataroot = '/local_raid3/03_user/jungmin/02_data/Encoding_decoding/encoding_analyzing/02_Goal-Driven-DL/01_NeuralEncodingDecoding/sourcecode/source_code_v2/source_code_v2/feature_extracted/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN activation \n",
    "1. training movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lay = 0\n",
    "\n",
    "for seg in range(1,19):\n",
    "    print('seg:' , str(seg), '; Layer: ',layername[lay])\n",
    "    secpath = (dataroot + 'AlexNet_feature_maps_processed_seg'+ str(seg) + '.h5')\n",
    "    lay_feat = h5py.File(secpath, 'r')\n",
    "    lay_feat = lay_feat[layername[lay] + '/data']\n",
    "    lay_feat = lay_feat.value\n",
    "    lay_feat = np.transpose(lay_feat,(3,2,1,0))\n",
    "    print('lay_feat: ',lay_feat.shape)\n",
    "    dim = lay_feat.shape\n",
    "    print('dim:', dim)\n",
    "    Nf = dim[-1]\n",
    "    print('Nf:', Nf)\n",
    "    if seg == 1:\n",
    "        dim_array = np.asarray(dim)\n",
    "        dim_array[-1] = Nf*18\n",
    "        print(dim_array)\n",
    "        lay_feat_concatenated = np.zeros(dim_array[0:4])\n",
    "        print('lay_feat_concatenated:',lay_feat_concatenated.shape)\n",
    "    lay_feat_concatenated[:,:,:,(seg-1)*Nf+0:seg*Nf] = lay_feat\n",
    "    print(lay_feat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. standarize time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = lay_feat_concatenated.shape\n",
    "dim= np.asarray(dim)\n",
    "lay_feat_concatenated = lay_feat_concatenated.reshape(np.prod(dim[:-1]),dim[-1],order='F').copy()\n",
    "lay_feat_concatenated_mean = np.mean(lay_feat_concatenated,1)\n",
    "lay_feat_concatenated_mean = np.expand_dims(lay_feat_concatenated_mean,-1)\n",
    "lay_feat_concatenated_std = lay_feat_concatenated.std(1)\n",
    "lay_feat_concatenated_std = np.expand_dims(lay_feat_concatenated_std,-1)\n",
    "lay_feat_concatenated = np.subtract(lay_feat_concatenated,lay_feat_concatenated_mean)\n",
    "lay_feat_concatenated = np.divide(lay_feat_concatenated,lay_feat_concatenated_std)\n",
    "np.nan_to_num(lay_feat_concatenated, copy=False)\n",
    "lay_feat_concatenated = lay_feat_concatenated.reshape(dim,order='F').copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scipy.io.savemat('AlexNet_feature_maps_process_layer1_concatenated.mat', lay_feat_concatenated, lay_feat_concatenated_mean, lay_feat_concatenated_std)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fMRI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check correlation with .mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. lay_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_lay_feat = loadmat('ori_lay_feat.mat')\n",
    "print(ori_lay_feat.keys())\n",
    "ori_lay_feat = ori_lay_feat['lay_feat']\n",
    "print(ori_lay_feat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lay_feat_list = []\n",
    "for i in range(100):\n",
    "    r = np.corrcoef(lay_feat[:,:,:,i].ravel(), ori_lay_feat[:,:,:,i].ravel())[0,1]\n",
    "    lay_feat_list.append(r)\n",
    "    \n",
    "print(np.mean(lay_feat_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(lay_feat[:,:,:,i].ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. lay_feat_concatenated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = '/local_raid3/03_user/jungmin/02_data/Encoding_decoding/encoding_analyzing/02_Goal-Driven-DL/01_NeuralEncodingDecoding/sourcecode/source_code_v2/source_code_v2/check_correaltion/lay_feat_concat_reshape.mat'\n",
    "lay_concat_reshape = h5py.File(dir , 'r')\n",
    "print(lay_concat_reshape.keys())\n",
    "lay_concat_reshape = lay_concat_reshape['lay_feat_concatenated']\n",
    "lay_concat_reshape = np.transpose(lay_concat_reshape,(1,0))\n",
    "print(lay_concat_reshape.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_list = []\n",
    "for i in range(100):\n",
    "    r = np.corrcoef(lay_feat_concatenated[:,i].ravel(), lay_concat_reshape[:,i].ravel())[0,1]\n",
    "    a_list.append(r)\n",
    "    \n",
    "print(np.mean(a_list))\n",
    "print(np.array_equal(lay_feat_concatenated, lay_concat_reshape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. lay_feat_concatenated_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lay_feat_concatenated_mean = np.mean(lay_feat_concatenated,1)\n",
    "lay_feat_concatenated_mean = np.expand_dims(lay_feat_concatenated_mean,-1)\n",
    "print(lay_feat_concatenated_mean.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = '/local_raid3/03_user/jungmin/02_data/Encoding_decoding/encoding_analyzing/02_Goal-Driven-DL/01_NeuralEncodingDecoding/sourcecode/source_code_v2/source_code_v2/check_correaltion/lay_feat_concat_mean.mat'\n",
    "lay_concat_mean = h5py.File(dir , 'r')\n",
    "print(lay_concat_mean.keys())\n",
    "lay_concat_mean = lay_concat_mean['lay_feat_concatenated_mean']\n",
    "lay_concat_mean = np.transpose(lay_concat_mean,(1,0))\n",
    "print(lay_concat_mean.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_list = []\n",
    "for i in range(100):\n",
    "    r = np.corrcoef(lay_feat_concatenated_mean[i:,].ravel(), lay_concat_mean[i:,].ravel())[0,1]\n",
    "    a_list.append(r)\n",
    "    \n",
    "print(np.mean(a_list))\n",
    "print(np.array_equal(lay_feat_concatenated_mean, lay_concat_mean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. lay_feat_concatenated_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lay_feat_concatenated_std = lay_feat_concatenated.std(1)\n",
    "lay_feat_concatenated_std = np.expand_dims(lay_feat_concatenated_std,-1)\n",
    "print(lay_feat_concatenated_std.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = '/local_raid3/03_user/jungmin/02_data/Encoding_decoding/encoding_analyzing/02_Goal-Driven-DL/01_NeuralEncodingDecoding/sourcecode/source_code_v2/source_code_v2/check_correaltion/lay_feat_concat_std.mat'\n",
    "lay_concat_std = h5py.File(dir , 'r')\n",
    "print(lay_concat_std.keys())\n",
    "lay_concat_std = lay_concat_std['lay_feat_concatenated_std']\n",
    "lay_concat_std = np.transpose(lay_concat_std,(1,0))\n",
    "print(lay_concat_std.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_list = []\n",
    "for i in range(100):\n",
    "    r = np.corrcoef(lay_feat_concatenated_std[i:,].ravel(), lay_concat_std[i:,].ravel())[0,1]\n",
    "    a_list.append(r)\n",
    "    \n",
    "print(np.mean(a_list))\n",
    "print(np.array_equal(lay_feat_concatenated_std, lay_concat_std))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. lay_feat_concatenated bsxfun@minus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lay_feat_concatenated = np.subtract(lay_feat_concatenated,lay_feat_concatenated_mean)\n",
    "print(lay_feat_concatenated.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = '/local_raid3/03_user/jungmin/02_data/Encoding_decoding/encoding_analyzing/02_Goal-Driven-DL/01_NeuralEncodingDecoding/sourcecode/source_code_v2/source_code_v2/check_correaltion/lay_feat_minus.mat'\n",
    "lay_concat_minus = h5py.File(dir , 'r')\n",
    "print(lay_concat_minus.keys())\n",
    "lay_concat_minus = lay_concat_minus['lay_feat_concatenated']\n",
    "lay_concat_minus = np.transpose(lay_concat_minus,(1,0))\n",
    "print(lay_concat_minus.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_list = []\n",
    "for i in range(100):\n",
    "    r = np.corrcoef(lay_feat_concatenated[:,i].ravel(), lay_concat_minus[:,i].ravel())[0,1]\n",
    "    a_list.append(r)\n",
    "    \n",
    "print(np.mean(a_list))\n",
    "print(np.array_equal(lay_feat_concatenated, lay_concat_minus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. lay_feat_concatenated bsxfun@rdivide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lay_feat_concatenated = np.divide(lay_feat_concatenated,lay_feat_concatenated_std)\n",
    "print(lay_feat_concatenated.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = '/local_raid3/03_user/jungmin/02_data/Encoding_decoding/encoding_analyzing/02_Goal-Driven-DL/01_NeuralEncodingDecoding/sourcecode/source_code_v2/source_code_v2/check_correaltion/lay_feat_divide.mat'\n",
    "lay_concat_divide = h5py.File(dir , 'r')\n",
    "print(lay_concat_divide.keys())\n",
    "lay_concat_divide = lay_concat_divide['lay_feat_concatenated']\n",
    "lay_concat_divide = np.transpose(lay_concat_divide,(1,0))\n",
    "print(lay_concat_divide.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_list = []\n",
    "for i in range(100):\n",
    "    r = np.corrcoef(lay_feat_concatenated[:,i].ravel(), lay_concat_divide[:,i].ravel())[0,1]\n",
    "    a_list.append(r)\n",
    "    \n",
    "print(np.mean(a_list))\n",
    "print(np.array_equal(lay_feat_concatenated, lay_concat_divide))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. lay_feat_concatenate isnan to zeros then reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.nan_to_num(lay_feat_concatenated, copy=False)\n",
    "print(lay_feat_concatenated.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = '/local_raid3/03_user/jungmin/02_data/Encoding_decoding/encoding_analyzing/02_Goal-Driven-DL/01_NeuralEncodingDecoding/sourcecode/source_code_v2/source_code_v2/check_correaltion/lay_feat_nan.mat'\n",
    "lay_nan = h5py.File(dir , 'r')\n",
    "print(lay_nan.keys())\n",
    "lay_nan = lay_nan['b']\n",
    "print(lay_nan.shape)\n",
    "lay_nan = np.transpose(lay_nan,(1,0))\n",
    "print(lay_nan.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_list = []\n",
    "for i in range(100):\n",
    "    r = np.corrcoef(lay_feat_concatenated[:,i].ravel(), lay_nan[:,i].ravel())[0,1]\n",
    "    a_list.append(r)\n",
    "    \n",
    "print(np.mean(a_list))\n",
    "print(np.array_equal(lay_feat_concatenated, lay_nan))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lay_feat_concatenated = lay_feat_concatenated.reshape(dim,order='F').copy()\n",
    "print(lay_feat_concatenated.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = '/local_raid3/03_user/jungmin/02_data/Encoding_decoding/encoding_analyzing/02_Goal-Driven-DL/01_NeuralEncodingDecoding/sourcecode/source_code_v2/source_code_v2/check_correaltion/lay_feat_nan2reshape.mat'\n",
    "lay_reshape_nan = h5py.File(dir , 'r')\n",
    "print(lay_reshape_nan.keys())\n",
    "lay_reshape_nan = lay_reshape_nan['lay_feat_concatenated']\n",
    "print(lay_reshape_nan.shape)\n",
    "lay_reshape_nan = np.transpose(lay_reshape_nan,(3,2,1,0))\n",
    "print(lay_reshape_nan.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_list = []\n",
    "for i in range(100):\n",
    "    r = np.corrcoef(lay_feat_concatenated[:,:,:,i].ravel(), lay_reshape_nan[:,:,:,i].ravel())[0,1]\n",
    "    a_list.append(r)\n",
    "    \n",
    "print(np.mean(a_list))\n",
    "print(np.array_equal(lay_feat_concatenated, lay_reshape_nan))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
